{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prepairing\" data-toc-modified-id=\"Prepairing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prepairing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-all-necessary\" data-toc-modified-id=\"Import-all-necessary-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import all necessary</a></span></li><li><span><a href=\"#Open-and-prepair-the-data\" data-toc-modified-id=\"Open-and-prepair-the-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Open and prepair the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Clean-up-text-messages-from-unnecessary-garbage\" data-toc-modified-id=\"Clean-up-text-messages-from-unnecessary-garbage-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Clean up text messages from unnecessary garbage</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Lemmatization</a></span></li></ul></li><li><span><a href=\"#Results-of-preparation\" data-toc-modified-id=\"Results-of-preparation-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Results of preparation</a></span></li></ul></li><li><span><a href=\"#Learning\" data-toc-modified-id=\"Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-list-of-stop-words\" data-toc-modified-id=\"Create-a-list-of-stop-words-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Create a list of stop words</a></span></li><li><span><a href=\"#Divide-the-data-and-define-features\" data-toc-modified-id=\"Divide-the-data-and-define-features-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Divide the data and define features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-the-features-and-goals-of-learning\" data-toc-modified-id=\"Define-the-features-and-goals-of-learning-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Define the features and goals of learning</a></span></li><li><span><a href=\"#Divide-the-samples\" data-toc-modified-id=\"Divide-the-samples-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Divide the samples</a></span></li></ul></li><li><span><a href=\"#Carry-out-TF-IDF-vectorization\" data-toc-modified-id=\"Carry-out-TF-IDF-vectorization-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Carry out TF-IDF vectorization</a></span></li><li><span><a href=\"#LogisticRegression\" data-toc-modified-id=\"LogisticRegression-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>LogisticRegression</a></span></li><li><span><a href=\"#DesicionTreeClassifier\" data-toc-modified-id=\"DesicionTreeClassifier-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>DesicionTreeClassifier</a></span></li><li><span><a href=\"#CatBoostClassifier\" data-toc-modified-id=\"CatBoostClassifier-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>CatBoostClassifier</a></span></li><li><span><a href=\"#Intermediate-conclusion\" data-toc-modified-id=\"Intermediate-conclusion-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Intermediate conclusion</a></span></li></ul></li><li><span><a href=\"#Model-testing\" data-toc-modified-id=\"Model-testing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model testing</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online store \"Wikishop\" launches a new service. Now users can edit and supplement product descriptions, just like in wiki communities. That is, clients propose their edits and comment on the changes of others. The store needs a tool that will look for toxic comments and submit them for moderation.\n",
    "\n",
    "Train the model to classify comments into positive and negative. At your disposal is a dataset with markup on the toxicity of edits.\n",
    "\n",
    "Build a model with a quality metric *F1* of at least 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepairing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pgore\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from dframcy import DframCy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "state = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and prepair the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv ('toxic_comments.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Description:\n",
    "1. Unnamed: 0 - column with old indexes from csv or excell file, can be deleted\n",
    "2. text - a column with text messages, there are capital letters and separators, this should also be removed.\n",
    "3. toxic - a column indicating whether the comment is toxic or not, is the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete column Unnamed:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns = ['Unnamed: 0'],axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up text messages from unnecessary garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    clear_text = re.sub(r'[^a-zA-Z]', ' ', text) \n",
    "    clear_text = clear_text.split()\n",
    "    clear_text = \" \".join(clear_text)\n",
    "    clear_text = clear_text.lower()\n",
    "    return clear_text\n",
    "\n",
    "\n",
    "data['clear_text'] = data['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                          clear_text  \n",
       "0  explanation why the edits made under my userna...  \n",
       "1  d aww he matches this background colour i m se...  \n",
       "2  hey man i m really not trying to edit war it s...  \n",
       "3  more i can t make any real suggestions on impr...  \n",
       "4  you sir are my hero any chance you remember wh...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframcy = DframCy(nlp)\n",
    "\n",
    "def lemmatize (text):\n",
    "    doc = dframcy.nlp (text)\n",
    "    #doc_df = dframcy.to_dataframe(doc)\n",
    "    #doc_df['index'] = text[0]\n",
    "    lemm_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemm_text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         explanation why the edit make under my usernam...\n",
       "1         d aww he matches this background colour i m se...\n",
       "2         hey man i m really not trying to edit war it s...\n",
       "3         more i can t make any real suggestions on impr...\n",
       "4         you sir are my hero any chance you remember wh...\n",
       "                                ...                        \n",
       "159287    and for the second time of asking when your vi...\n",
       "159288    you should be ashamed of yourself that is a ho...\n",
       "159289    spitzer umm theres no actual article for prost...\n",
       "159290    and it looks like it was actually you who put ...\n",
       "159291    and i really don t think you understand i came...\n",
       "Name: clear_text, Length: 159292, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data['clear_text'].copy()\n",
    "corpus[0] = lemmatize(corpus[0])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>explanation why the edit make under my usernam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>and i really don t think you understand i came...</td>\n",
       "      <td>and i really don t think you understand i came...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159287  \":::::And for the second time of asking, when ...      0   \n",
       "159288  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159289  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159290  And it looks like it was actually you who put ...      0   \n",
       "159291  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                                               clear_text  \\\n",
       "0       explanation why the edits made under my userna...   \n",
       "1       d aww he matches this background colour i m se...   \n",
       "2       hey man i m really not trying to edit war it s...   \n",
       "3       more i can t make any real suggestions on impr...   \n",
       "4       you sir are my hero any chance you remember wh...   \n",
       "...                                                   ...   \n",
       "159287  and for the second time of asking when your vi...   \n",
       "159288  you should be ashamed of yourself that is a ho...   \n",
       "159289  spitzer umm theres no actual article for prost...   \n",
       "159290  and it looks like it was actually you who put ...   \n",
       "159291  and i really don t think you understand i came...   \n",
       "\n",
       "                                                lemm_text  \n",
       "0       explanation why the edit make under my usernam...  \n",
       "1       d aww he matches this background colour i m se...  \n",
       "2       hey man i m really not trying to edit war it s...  \n",
       "3       more i can t make any real suggestions on impr...  \n",
       "4       you sir are my hero any chance you remember wh...  \n",
       "...                                                   ...  \n",
       "159287  and for the second time of asking when your vi...  \n",
       "159288  you should be ashamed of yourself that is a ho...  \n",
       "159289  spitzer umm theres no actual article for prost...  \n",
       "159290  and it looks like it was actually you who put ...  \n",
       "159291  and i really don t think you understand i came...  \n",
       "\n",
       "[159292 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemm_text'] = pd.Series(corpus)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Removed extra column.\n",
    "2. Texts are cleared of unnecessary characters and capital letters.\n",
    "3. Texts are lemmatized and prepared for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the data and define features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the features and goals of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['toxic']\n",
    "features = data['lemm_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127433, 15929, 15930)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size = 0.2, shuffle = True, random_state = state)\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(\n",
    "    features_valid, target_valid, shuffle = True, test_size = 0.5, random_state = state)\n",
    "\n",
    "features_train.shape[0], features_valid.shape[0], features_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carry out TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf = tf_idf.fit_transform(features_train)\n",
    "valid_tf_idf = tf_idf.transform(features_valid)\n",
    "test_tf_idf = tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on LR = 0.778\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(max_iter = 1000, class_weight = 'balanced', C = 10)\n",
    "LR.fit(train_tf_idf, target_train)\n",
    "LR_pred_valid = LR.predict(valid_tf_idf)\n",
    "LR_F1 = f1_score(target_valid, LR_pred_valid)\n",
    "print ('f1_score on LR =', round (LR_F1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DesicionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on DTC = 0.626\n"
     ]
    }
   ],
   "source": [
    "DTC = DecisionTreeClassifier(random_state = state, class_weight = 'balanced')\n",
    "DTC.fit(train_tf_idf, target_train)\n",
    "DTC_pred_valid = DTC.predict(valid_tf_idf)\n",
    "DTC_F1 = f1_score(target_valid, DTC_pred_valid)\n",
    "print ('f1_score on DTC =', round (DTC_F1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3450303\ttotal: 1.08s\tremaining: 52.8s\n",
      "5:\tlearn: 0.2066477\ttotal: 5.48s\tremaining: 40.2s\n",
      "10:\tlearn: 0.1845820\ttotal: 9.93s\tremaining: 35.2s\n",
      "15:\tlearn: 0.1727016\ttotal: 14.3s\tremaining: 30.3s\n",
      "20:\tlearn: 0.1651376\ttotal: 18.7s\tremaining: 25.8s\n",
      "25:\tlearn: 0.1583897\ttotal: 23.3s\tremaining: 21.5s\n",
      "30:\tlearn: 0.1525723\ttotal: 27.7s\tremaining: 17s\n",
      "35:\tlearn: 0.1480318\ttotal: 32.1s\tremaining: 12.5s\n",
      "40:\tlearn: 0.1439869\ttotal: 36.5s\tremaining: 8.01s\n",
      "45:\tlearn: 0.1407948\ttotal: 40.8s\tremaining: 3.55s\n",
      "49:\tlearn: 0.1377563\ttotal: 44.2s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x19ce0aef7c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBC = CatBoostClassifier(iterations = 50, learning_rate = 0.5, \n",
    "                           verbose = 5, random_state = state)\n",
    "CBC.fit(train_tf_idf, target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on CatBoost = 0.74\n"
     ]
    }
   ],
   "source": [
    "CBC_pred_valid = CBC.predict(valid_tf_idf)\n",
    "CBC_F1 = f1_score(target_valid, CBC_pred_valid)\n",
    "print ('f1_score on CatBoost =', round (CBC_F1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression was chosen as the best model, since its F1 metric is greater than the lower threshold of 0.76 and the training time is the shortest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the chosen model on the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score = 0.771\n"
     ]
    }
   ],
   "source": [
    "LR_pred_test = LR.predict(test_tf_idf)\n",
    "LR_F1_test = f1_score(target_test, LR_pred_test)\n",
    "print ('f1_score =', round (LR_F1_test,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The data was prepared, it was cleaned from unnecessary characters and lemmatization was carried out.\n",
    "2. A list of stop words is created, the data is divided into learning features and a target feature.\n",
    "3. TF-IDF vectorization of features for training was carried out.\n",
    "4. Trained the Logistic Regression model. F1 on the validation set - 0.778.\n",
    "5. The classification decision tree model was trained. F1 on the validation selection - 0.626\n",
    "6. Trained the CatBoostClassifier model. F1 on the validation set is 0.74, but the training time (compared to Regression) is very long.\n",
    "7. The Logistic Regression model was recognized as the best model.\n",
    "8. F1 of the best model on the test set is 0.771."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 410,
    "start_time": "2022-11-22T21:19:57.383Z"
   },
   {
    "duration": 3398,
    "start_time": "2022-11-22T21:20:11.019Z"
   },
   {
    "duration": 820,
    "start_time": "2022-11-22T21:20:19.028Z"
   },
   {
    "duration": 748,
    "start_time": "2022-11-22T21:20:29.169Z"
   },
   {
    "duration": 51,
    "start_time": "2022-11-23T08:40:37.708Z"
   },
   {
    "duration": 6,
    "start_time": "2022-11-23T08:40:40.714Z"
   },
   {
    "duration": 430,
    "start_time": "2022-11-23T08:40:43.113Z"
   },
   {
    "duration": 2390,
    "start_time": "2022-11-23T08:40:44.117Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-23T08:40:48.546Z"
   },
   {
    "duration": 41,
    "start_time": "2022-11-23T08:40:56.204Z"
   },
   {
    "duration": 10,
    "start_time": "2022-11-23T08:41:07.435Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-23T08:41:15.080Z"
   },
   {
    "duration": 829,
    "start_time": "2022-11-23T08:41:15.085Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-23T08:41:18.173Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-23T08:41:22.593Z"
   },
   {
    "duration": 807,
    "start_time": "2022-11-23T08:41:23.484Z"
   },
   {
    "duration": 12,
    "start_time": "2022-11-23T08:41:26.533Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-23T08:41:40.532Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-23T08:41:48.428Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-23T08:41:52.504Z"
   },
   {
    "duration": 838,
    "start_time": "2022-11-23T08:41:52.509Z"
   },
   {
    "duration": 14,
    "start_time": "2022-11-23T08:41:54.552Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-23T08:43:02.186Z"
   },
   {
    "duration": 12,
    "start_time": "2022-11-23T08:44:48.523Z"
   },
   {
    "duration": 2535,
    "start_time": "2022-11-23T08:44:54.349Z"
   },
   {
    "duration": 9,
    "start_time": "2022-11-23T08:45:03.207Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T08:45:10.017Z"
   },
   {
    "duration": 2,
    "start_time": "2022-11-23T08:46:07.599Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-23T08:47:25.619Z"
   },
   {
    "duration": 2875,
    "start_time": "2022-11-23T08:48:11.626Z"
   },
   {
    "duration": 446,
    "start_time": "2022-11-23T08:49:01.570Z"
   },
   {
    "duration": 962,
    "start_time": "2022-11-23T08:49:02.018Z"
   },
   {
    "duration": 20,
    "start_time": "2022-11-23T08:49:02.981Z"
   },
   {
    "duration": 2483,
    "start_time": "2022-11-23T08:49:03.003Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T08:49:05.488Z"
   },
   {
    "duration": 2766,
    "start_time": "2022-11-23T08:49:05.497Z"
   },
   {
    "duration": 404,
    "start_time": "2022-11-23T08:49:31.038Z"
   },
   {
    "duration": 2553,
    "start_time": "2022-11-23T08:49:31.444Z"
   },
   {
    "duration": 16,
    "start_time": "2022-11-23T08:49:33.999Z"
   },
   {
    "duration": 2458,
    "start_time": "2022-11-23T08:49:34.017Z"
   },
   {
    "duration": 8,
    "start_time": "2022-11-23T08:49:36.477Z"
   },
   {
    "duration": 2818,
    "start_time": "2022-11-23T08:49:36.486Z"
   },
   {
    "duration": 437,
    "start_time": "2022-11-23T08:50:05.262Z"
   },
   {
    "duration": 2439,
    "start_time": "2022-11-23T08:50:05.702Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-23T08:50:08.143Z"
   },
   {
    "duration": 2466,
    "start_time": "2022-11-23T08:50:08.160Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-23T08:50:10.628Z"
   },
   {
    "duration": 2837,
    "start_time": "2022-11-23T08:50:10.641Z"
   },
   {
    "duration": 84,
    "start_time": "2022-11-23T08:51:12.809Z"
   },
   {
    "duration": 1183,
    "start_time": "2022-11-23T08:51:25.622Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-23T08:51:54.020Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-23T08:51:55.970Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-23T08:51:56.590Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-23T08:52:02.782Z"
   },
   {
    "duration": 68,
    "start_time": "2022-11-23T08:52:17.410Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-23T08:52:47.315Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-23T08:52:51.407Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-23T08:54:47.855Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-23T08:57:13.527Z"
   },
   {
    "duration": 10,
    "start_time": "2022-11-23T09:00:09.681Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T09:00:29.332Z"
   },
   {
    "duration": 80,
    "start_time": "2022-11-23T09:00:32.732Z"
   },
   {
    "duration": 29675,
    "start_time": "2022-11-23T09:32:47.625Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-23T09:33:23.746Z"
   },
   {
    "duration": 865,
    "start_time": "2022-11-23T09:33:23.753Z"
   },
   {
    "duration": 14,
    "start_time": "2022-11-23T09:33:24.620Z"
   },
   {
    "duration": 2603,
    "start_time": "2022-11-23T09:33:24.636Z"
   },
   {
    "duration": 6,
    "start_time": "2022-11-23T09:33:27.241Z"
   },
   {
    "duration": 54650,
    "start_time": "2022-11-23T09:33:29.293Z"
   },
   {
    "duration": 1460,
    "start_time": "2022-11-23T09:34:46.240Z"
   },
   {
    "duration": 2467,
    "start_time": "2022-11-23T09:34:47.702Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-23T09:34:50.171Z"
   },
   {
    "duration": 2553,
    "start_time": "2022-11-23T09:34:50.188Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T09:34:52.743Z"
   },
   {
    "duration": 245822,
    "start_time": "2022-11-23T09:35:02.058Z"
   },
   {
    "duration": 2754,
    "start_time": "2022-11-23T09:39:11.315Z"
   },
   {
    "duration": 1603,
    "start_time": "2022-11-23T09:39:51.439Z"
   },
   {
    "duration": 2408,
    "start_time": "2022-11-23T09:39:53.044Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-23T09:39:55.454Z"
   },
   {
    "duration": 2521,
    "start_time": "2022-11-23T09:39:55.470Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T09:39:57.994Z"
   },
   {
    "duration": 2925,
    "start_time": "2022-11-23T09:39:58.003Z"
   },
   {
    "duration": 48,
    "start_time": "2022-11-23T09:40:07.857Z"
   },
   {
    "duration": 13,
    "start_time": "2022-11-23T09:40:11.721Z"
   },
   {
    "duration": 1450,
    "start_time": "2022-11-23T09:40:15.059Z"
   },
   {
    "duration": 2505,
    "start_time": "2022-11-23T09:40:16.512Z"
   },
   {
    "duration": 18,
    "start_time": "2022-11-23T09:40:19.018Z"
   },
   {
    "duration": 2510,
    "start_time": "2022-11-23T09:40:19.038Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T09:40:21.550Z"
   },
   {
    "duration": 2911,
    "start_time": "2022-11-23T09:40:21.558Z"
   },
   {
    "duration": 1477,
    "start_time": "2022-11-23T09:40:45.077Z"
   },
   {
    "duration": 2430,
    "start_time": "2022-11-23T09:40:46.556Z"
   },
   {
    "duration": 14,
    "start_time": "2022-11-23T09:40:48.988Z"
   },
   {
    "duration": 2496,
    "start_time": "2022-11-23T09:40:49.003Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T09:40:51.501Z"
   },
   {
    "duration": 114,
    "start_time": "2022-11-23T09:41:50.689Z"
   },
   {
    "duration": 9,
    "start_time": "2022-11-23T09:42:00.207Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-23T09:42:03.238Z"
   },
   {
    "duration": 876,
    "start_time": "2022-11-23T09:42:03.244Z"
   },
   {
    "duration": 19,
    "start_time": "2022-11-23T09:42:04.122Z"
   },
   {
    "duration": 2694,
    "start_time": "2022-11-23T09:42:04.145Z"
   },
   {
    "duration": 8,
    "start_time": "2022-11-23T09:42:06.841Z"
   },
   {
    "duration": 2943,
    "start_time": "2022-11-23T09:42:06.851Z"
   },
   {
    "duration": 144809,
    "start_time": "2022-11-23T09:42:09.797Z"
   },
   {
    "duration": 1045,
    "start_time": "2022-11-23T09:49:05.018Z"
   },
   {
    "duration": 116534,
    "start_time": "2022-11-23T09:51:08.517Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-23T10:09:46.886Z"
   },
   {
    "duration": 19,
    "start_time": "2022-11-23T10:09:46.894Z"
   },
   {
    "duration": 1060,
    "start_time": "2022-11-23T10:09:46.915Z"
   },
   {
    "duration": 21,
    "start_time": "2022-11-23T10:09:47.977Z"
   },
   {
    "duration": 2058,
    "start_time": "2022-11-23T10:09:52.824Z"
   },
   {
    "duration": 1680,
    "start_time": "2022-11-23T10:10:58.763Z"
   },
   {
    "duration": 13,
    "start_time": "2022-11-23T10:11:02.706Z"
   },
   {
    "duration": 1567,
    "start_time": "2022-11-23T10:12:46.581Z"
   },
   {
    "duration": 1225,
    "start_time": "2022-11-23T10:12:48.151Z"
   },
   {
    "duration": 2630,
    "start_time": "2022-11-23T10:12:49.378Z"
   },
   {
    "duration": 23,
    "start_time": "2022-11-23T10:12:52.011Z"
   },
   {
    "duration": 235,
    "start_time": "2022-11-23T10:12:52.040Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-23T10:12:52.277Z"
   },
   {
    "duration": 4152,
    "start_time": "2022-11-23T10:13:07.813Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T10:13:14.204Z"
   },
   {
    "duration": 3660,
    "start_time": "2022-11-23T10:13:54.791Z"
   },
   {
    "duration": 10,
    "start_time": "2022-11-23T10:13:58.453Z"
   },
   {
    "duration": 3162,
    "start_time": "2022-11-23T10:14:04.009Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-23T10:16:00.003Z"
   },
   {
    "duration": 898,
    "start_time": "2022-11-23T10:19:29.725Z"
   },
   {
    "duration": 912,
    "start_time": "2022-11-23T10:19:39.416Z"
   },
   {
    "duration": 755,
    "start_time": "2022-11-23T10:19:59.766Z"
   },
   {
    "duration": 1870,
    "start_time": "2022-11-23T10:20:11.825Z"
   },
   {
    "duration": 1112,
    "start_time": "2022-11-23T10:20:13.699Z"
   },
   {
    "duration": 31,
    "start_time": "2022-11-23T10:20:14.815Z"
   },
   {
    "duration": 4783,
    "start_time": "2022-11-23T10:20:14.848Z"
   },
   {
    "duration": 20,
    "start_time": "2022-11-23T10:20:19.635Z"
   },
   {
    "duration": 3259,
    "start_time": "2022-11-23T10:20:19.657Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T10:20:32.775Z"
   },
   {
    "duration": 955,
    "start_time": "2022-11-23T10:20:34.055Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-23T10:20:37.262Z"
   },
   {
    "duration": 4530,
    "start_time": "2022-11-23T10:20:38.496Z"
   },
   {
    "duration": 6,
    "start_time": "2022-11-23T10:20:43.766Z"
   },
   {
    "duration": 1558,
    "start_time": "2022-11-23T10:21:03.094Z"
   },
   {
    "duration": 3134,
    "start_time": "2022-11-23T10:21:04.656Z"
   },
   {
    "duration": 17,
    "start_time": "2022-11-23T10:21:07.791Z"
   },
   {
    "duration": 4492,
    "start_time": "2022-11-23T10:21:07.809Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T10:21:12.304Z"
   },
   {
    "duration": 3270,
    "start_time": "2022-11-23T10:21:12.313Z"
   },
   {
    "duration": 800,
    "start_time": "2022-11-23T10:21:15.586Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T10:21:16.389Z"
   },
   {
    "duration": 8,
    "start_time": "2022-11-23T10:21:16.397Z"
   },
   {
    "duration": 95,
    "start_time": "2022-11-23T10:21:16.406Z"
   },
   {
    "duration": 10724,
    "start_time": "2022-11-23T10:21:16.503Z"
   },
   {
    "duration": 786,
    "start_time": "2022-11-23T10:28:36.541Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-23T10:28:42.616Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-23T10:28:44.459Z"
   },
   {
    "duration": 42,
    "start_time": "2022-11-23T10:28:45.498Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-23T10:29:22.239Z"
   },
   {
    "duration": 6914,
    "start_time": "2022-11-23T10:31:25.158Z"
   },
   {
    "duration": 7948,
    "start_time": "2022-11-23T10:31:43.833Z"
   },
   {
    "duration": 8521,
    "start_time": "2022-11-23T10:31:55.570Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-23T10:32:57.108Z"
   },
   {
    "duration": 26,
    "start_time": "2022-11-23T10:33:35.904Z"
   },
   {
    "duration": 30628,
    "start_time": "2022-11-23T10:33:55.519Z"
   },
   {
    "duration": 59607,
    "start_time": "2022-11-23T10:34:29.932Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-23T10:36:01.524Z"
   },
   {
    "duration": 56149,
    "start_time": "2022-11-23T10:43:20.195Z"
   },
   {
    "duration": 12,
    "start_time": "2022-11-23T10:44:35.295Z"
   },
   {
    "duration": 17,
    "start_time": "2022-11-23T10:47:58.141Z"
   },
   {
    "duration": 39,
    "start_time": "2022-11-23T10:48:24.221Z"
   },
   {
    "duration": 3044,
    "start_time": "2022-11-23T10:48:28.871Z"
   },
   {
    "duration": 40,
    "start_time": "2022-11-23T10:48:51.593Z"
   },
   {
    "duration": 16,
    "start_time": "2022-11-23T10:48:53.653Z"
   },
   {
    "duration": 7667,
    "start_time": "2022-11-23T10:48:54.364Z"
   },
   {
    "duration": 66691,
    "start_time": "2022-11-23T10:49:03.168Z"
   },
   {
    "duration": 8,
    "start_time": "2022-11-23T10:50:11.417Z"
   },
   {
    "duration": 108,
    "start_time": "2022-11-23T10:50:55.178Z"
   },
   {
    "duration": 12,
    "start_time": "2022-11-23T10:50:58.439Z"
   },
   {
    "duration": 3371,
    "start_time": "2022-11-23T10:51:02.813Z"
   },
   {
    "duration": 3397,
    "start_time": "2022-11-23T10:51:23.335Z"
   },
   {
    "duration": 3341,
    "start_time": "2022-11-23T10:51:31.925Z"
   },
   {
    "duration": 3274,
    "start_time": "2022-11-23T10:52:10.417Z"
   },
   {
    "duration": 186,
    "start_time": "2022-11-23T10:52:49.045Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T10:53:30.193Z"
   },
   {
    "duration": 189,
    "start_time": "2022-11-23T10:53:37.600Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-23T10:53:38.327Z"
   },
   {
    "duration": 24,
    "start_time": "2022-11-23T10:53:46.757Z"
   },
   {
    "duration": 10,
    "start_time": "2022-11-23T10:53:50.315Z"
   },
   {
    "duration": 85273,
    "start_time": "2022-11-23T10:54:04.855Z"
   },
   {
    "duration": 214,
    "start_time": "2022-11-23T10:56:34.053Z"
   },
   {
    "duration": 9,
    "start_time": "2022-11-23T10:56:35.052Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-23T10:56:49.837Z"
   },
   {
    "duration": 495531,
    "start_time": "2022-11-23T10:58:58.501Z"
   },
   {
    "duration": 165490,
    "start_time": "2022-11-23T11:07:33.008Z"
   },
   {
    "duration": 583275,
    "start_time": "2022-11-23T11:11:02.251Z"
   },
   {
    "duration": 48,
    "start_time": "2022-11-24T08:01:42.981Z"
   },
   {
    "duration": 1524,
    "start_time": "2022-11-24T08:01:49.243Z"
   },
   {
    "duration": 115,
    "start_time": "2022-11-24T08:01:50.769Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-24T08:01:50.886Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-24T08:01:50.887Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-24T08:01:50.888Z"
   },
   {
    "duration": 658,
    "start_time": "2022-11-24T10:36:19.417Z"
   },
   {
    "duration": 7782,
    "start_time": "2022-11-24T10:36:25.032Z"
   },
   {
    "duration": 656,
    "start_time": "2022-11-24T10:36:34.689Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.388px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
